# タイトル
Train longer, generalize better: closing the generalization gap in large batch training of neural networks [arxiv](https://arxiv.org/abs/1705.08741)

# どんなもの？

## 背景
確率的勾配法や亜種で訓練する際に，大きなバッチサイズを用いると小さなバッチサイズを用いるよりも汎化性能が低下すること（汎化ギャップ現象）が知られている．

## 動機
大きなバッチサイズでの訓練は，確率的勾配法や亜種の並列化を可能にし，訓練時間の短縮につながる．

## 貢献
確率的勾配法の振舞を「random walk on a random potential」でモデル化すると「パラメータが初期値からどれだけ離れるかが更新回数の対数に比例する」ことを発見した．
このモデルを用いて訓練課程を分析することで，汎化ギャップ現象の原因を特定し，汎化ギャップ現象を解消する訓練手法を開発した．
その訓練手法とは，次の３つを組み合わせたものである．

- Learning rate tuning
- Ghost Batch Normalization
- Regime adaptation

# 先行研究と比べてどこがすごい？
確率的勾配法の振舞を「random walk on a random potential」でモデル化した．
小さなバッチサイズを用いた訓練から生まれる大きな推定ノイズを，大きなバッチサイズを用いた訓練で再現することで，汎化ギャップ現象を解消した（小さなバッチサイズよりも汎化性能を高められた）．

# 技術や手法の肝はどこ？
小さなバッチサイズを用いた訓練から生まれる大きな推定ノイズを次の２つの手法で再現した。

- Learning rate tuning: バッチサイズの平方根に比例するように学習率を定める．
- Ghost Batch Normalization: 大きなバッチを分割した仮想バッチごとに Batch Normalization を行う．

# どうやって有効だと検証した？
大きなバッチサイズに関する Keskar らの先行研究では次のような観測が得られた．

- 大きなバッチサイズを用いた訓練は汎化誤差（汎化ギャップ）を大きくする．
- 損失が減らなくなるまで訓練を行っても汎化ギャップが解消されないように見える．
- 低い汎化性能は「sharp minima（鋭い窪み）」と関係し，良い汎化性能は「flat minima（平らな窪み）」と関係する．
- 小さなバッチサイズを用いた訓練は大きなバッチサイズを用いた訓練よりも初期値から遠くへパラメータが移動している．

この観測から，彼らは次の仮説を立てた．

- 小さなバッチサイズを用いた訓練から生まれる大きな推定ノイズが「sharp minima」を乗り越え「flat minima」まで導くため，よい汎化性能となる．

これに対して，著者は次の仮説を立てた．

- 大きな推定ノイズが鍵となるのであれば，同等の推定ノイズを再現して同等の回数だけ更新すれば汎化ギャップを解消できる．

確率的勾配法の振舞を「random walk on a random potential」でモデル化することで次を導出した．

- パラメータの移動量（汎化性能に対する緩い指標）が更新回数の対数に比例する（バッチサイズに依存していない）．⇒ Regime adaptation
- パラメータの移動量を一定にする場合，学習率がバッチサイズの平方根に比例する．⇒ Learning rate tuning

これにより，学習率を高くして（Learning rate tuning）更新回数を揃え（Regime adaptation）れば良いことが分かった．
ただし，バッチサイズの違いに起因する分散（推定ノイズ）の大きさについては，これだけでは再現できないので，大きなバッチを分割した仮想バッチごとに Batch Normalization を行う手法を考案した．

これらを用いることで，小さなバッチサイズを用いた訓練から生まれる大きな推定ノイズを大きなバッチサイズでも再現できることを実験により確認した．

# 議論はある？
確率的勾配法の振舞についてより深い理解が得られた．個人的にはコレが最も重要な仕事だと思う．
大きなバッチサイズを用いた訓練であっても汎化ギャップが生じないようにはできたが，訓練時間の短縮には至っていない（むしろ大幅に増えている）．

Ghost Batch Normalization については，アイデアは面白いが，あまり機能していないようにも思える。

というかアルゴリズムの記述がおかしい気がする．
記述が正しいなら，仮想バッチの統計量に掛けられている係数の減衰が速すぎて、ほぼ１つ目の仮想バッチしか意味を持たないような状況になっているように見える．
この懸念通りなら，大きなバッチサイズを扱ってはいるとは言い難く，Ghost Batch Normalization については意味を成していない．

Github の実装を見る感じでは，記述とはちょっと違う実装になっているように見える．
私が PyTorch について詳しくないので正確には分からないが，単純に仮想バッチごとに Batch Normalization して，各統計量を平均したものをバッチ全体の統計量として移動平均で足しているだけ？

将来的には速度も改善してゆくと書いてあるが、「パラメータの移動量が更新回数の対数に比例する」という理論的な結論から考えると改善するのは難しいように思える（バッチサイズとは無関係にパラメータの移動量が決まるなら，バッチサイズは小さい方が計算効率がよいため）．
バッチサイズ大きくして汎化性能を高めるという方向性は有るので、そっち方向の研究にしてゆくのが自然な流れのように思える．
そうして場合，同じ汎化性能を達成するのにかかる計算量が小さくできるということはあり得るかもしれないが，ちょっと難しいように思える，


# 次に読むべき論文は？
- Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training for deep learning: Generalization gap and sharp minima. In ICLR, 2017.
